Eval num_timesteps=1000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 1    |
|    iterations      | 1    |
|    time_elapsed    | 1109 |
|    total_timesteps | 2048 |
-----------------------------
Early stopping at step 2 due to reaching max kl: 0.46
Eval num_timesteps=3000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 3000       |
| train/                  |            |
|    approx_kl            | 0.26356336 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.56      |
|    explained_variance   | -0.00481   |
|    learning_rate        | 0.0001     |
|    loss                 | 27.7       |
|    n_updates            | 3          |
|    policy_gradient_loss | 0.0797     |
|    std                  | 0.999      |
|    value_loss           | 65.4       |
----------------------------------------
Eval num_timesteps=4000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 1    |
|    iterations      | 2    |
|    time_elapsed    | 2199 |
|    total_timesteps | 4096 |
-----------------------------
Early stopping at step 2 due to reaching max kl: 0.49
Eval num_timesteps=5000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.40383345 |
|    clip_fraction        | 0.504      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.851     |
|    explained_variance   | -0.414     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.898      |
|    n_updates            | 6          |
|    policy_gradient_loss | 0.066      |
|    std                  | 0.998      |
|    value_loss           | 12.3       |
----------------------------------------
Eval num_timesteps=6000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 6144
Recent rewards: [1.7210121552149455, 1.7210050026575725, 1.7210041284561157]
Reward changes: [-7.15255737e-06 -8.74201457e-07]
[Exploration Jump] Env 0 triggered.
-----------------------------
| time/              |      |
|    fps             | 1    |
|    iterations      | 3    |
|    time_elapsed    | 3288 |
|    total_timesteps | 6144 |
-----------------------------
Early stopping at step 0 due to reaching max kl: 0.47
Eval num_timesteps=7000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 7000       |
| train/                  |            |
|    approx_kl            | 0.18294084 |
|    clip_fraction        | 0.651      |
|    clip_range           | 0.2        |
|    entropy_loss         | 3.2        |
|    explained_variance   | -1.78      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.97       |
|    n_updates            | 7          |
|    policy_gradient_loss | 0.0726     |
|    std                  | 0.997      |
|    value_loss           | 1.15       |
----------------------------------------
Eval num_timesteps=8000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 8192
Recent rewards: [1.7210050026575725, 1.7210041284561157, 1.7209843794504802]
Reward changes: [-8.74201457e-07 -1.97490056e-05]
[Exploration Jump] Env 0 triggered.
-----------------------------
| time/              |      |
|    fps             | 1    |
|    iterations      | 4    |
|    time_elapsed    | 4383 |
|    total_timesteps | 8192 |
-----------------------------
Early stopping at step 0 due to reaching max kl: 0.47
Eval num_timesteps=9000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 9000       |
| train/                  |            |
|    approx_kl            | 0.13473848 |
|    clip_fraction        | 0.477      |
|    clip_range           | 0.2        |
|    entropy_loss         | 10         |
|    explained_variance   | -0.539     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.69       |
|    n_updates            | 8          |
|    policy_gradient_loss | 0.119      |
|    std                  | 0.996      |
|    value_loss           | 1.33       |
----------------------------------------
Eval num_timesteps=10000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 10240
Recent rewards: [1.7210041284561157, 1.7209843794504802, 1.7209813594818115]
Reward changes: [-1.97490056e-05 -3.01996867e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 5     |
|    time_elapsed    | 5491  |
|    total_timesteps | 10240 |
------------------------------
Early stopping at step 0 due to reaching max kl: 0.52
Eval num_timesteps=11000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 11000      |
| train/                  |            |
|    approx_kl            | 0.18746565 |
|    clip_fraction        | 0.71       |
|    clip_range           | 0.2        |
|    entropy_loss         | 17         |
|    explained_variance   | -0.359     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.61       |
|    n_updates            | 9          |
|    policy_gradient_loss | 0.0868     |
|    std                  | 0.995      |
|    value_loss           | 1.6        |
----------------------------------------
Eval num_timesteps=12000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 12288
Recent rewards: [1.7209843794504802, 1.7209813594818115, 1.7209759950637817]
Reward changes: [-3.01996867e-06 -5.36441803e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 6     |
|    time_elapsed    | 6592  |
|    total_timesteps | 12288 |
------------------------------
Early stopping at step 1 due to reaching max kl: 0.47
Eval num_timesteps=13000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 13000      |
| train/                  |            |
|    approx_kl            | 0.38278586 |
|    clip_fraction        | 0.76       |
|    clip_range           | 0.2        |
|    entropy_loss         | 25         |
|    explained_variance   | -0.225     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.83       |
|    n_updates            | 11         |
|    policy_gradient_loss | 0.0945     |
|    std                  | 0.992      |
|    value_loss           | 1.41       |
----------------------------------------
Eval num_timesteps=14000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 14336
Recent rewards: [1.7209813594818115, 1.7209759950637817, 1.7209100723266602]
Reward changes: [-5.36441803e-06 -6.59227371e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 7     |
|    time_elapsed    | 7671  |
|    total_timesteps | 14336 |
------------------------------
Early stopping at step 2 due to reaching max kl: 0.45
Eval num_timesteps=15000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.43474784 |
|    clip_fraction        | 0.872      |
|    clip_range           | 0.2        |
|    entropy_loss         | 31.6       |
|    explained_variance   | -0.224     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.74       |
|    n_updates            | 14         |
|    policy_gradient_loss | 0.121      |
|    std                  | 0.987      |
|    value_loss           | 0.942      |
----------------------------------------
Eval num_timesteps=16000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 16384
Recent rewards: [1.7209759950637817, 1.7209100723266602, 1.7209434111913045]
Reward changes: [-6.59227371e-05  3.33388646e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 8     |
|    time_elapsed    | 8749  |
|    total_timesteps | 16384 |
------------------------------
Early stopping at step 2 due to reaching max kl: 0.46
Eval num_timesteps=17000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 17000      |
| train/                  |            |
|    approx_kl            | 0.42923343 |
|    clip_fraction        | 0.913      |
|    clip_range           | 0.2        |
|    entropy_loss         | 36.1       |
|    explained_variance   | -0.136     |
|    learning_rate        | 0.0001     |
|    loss                 | 1.62       |
|    n_updates            | 17         |
|    policy_gradient_loss | 0.128      |
|    std                  | 0.982      |
|    value_loss           | 0.805      |
----------------------------------------
Eval num_timesteps=18000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 18432
Recent rewards: [1.7209100723266602, 1.7209434111913045, 1.7209316889444988]
Reward changes: [ 3.33388646e-05 -1.17222468e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 9     |
|    time_elapsed    | 9861  |
|    total_timesteps | 18432 |
------------------------------
Eval num_timesteps=19000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.72       |
| time/                   |            |
|    total_timesteps      | 19000      |
| train/                  |            |
|    approx_kl            | 0.21429059 |
|    clip_fraction        | 0.968      |
|    clip_range           | 0.2        |
|    entropy_loss         | 36         |
|    explained_variance   | -0.0473    |
|    learning_rate        | 0.0001     |
|    loss                 | 1.96       |
|    n_updates            | 25         |
|    policy_gradient_loss | 0.144      |
|    std                  | 0.96       |
|    value_loss           | 1.05       |
----------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!

[Plateau Detected] Forcing Exploration Jump at step 20480
Recent rewards: [1.7209434111913045, 1.7209316889444988, 1.7210182746251423]
Reward changes: [-1.17222468e-05  8.65856806e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 10    |
|    time_elapsed    | 10977 |
|    total_timesteps | 20480 |
------------------------------
Eval num_timesteps=21000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.028235406 |
|    clip_fraction        | 0.897       |
|    clip_range           | 0.2         |
|    entropy_loss         | 35.9        |
|    explained_variance   | -0.0255     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.87        |
|    n_updates            | 33          |
|    policy_gradient_loss | 0.0326      |
|    std                  | 0.937       |
|    value_loss           | 1.07        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=22000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 22528
Recent rewards: [1.7209316889444988, 1.7210182746251423, 1.7210275332132976]
Reward changes: [8.65856806e-05 9.25858816e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 11    |
|    time_elapsed    | 12049 |
|    total_timesteps | 22528 |
------------------------------
Eval num_timesteps=23000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.029954698 |
|    clip_fraction        | 0.8         |
|    clip_range           | 0.2         |
|    entropy_loss         | 39.6        |
|    explained_variance   | -0.0224     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.68        |
|    n_updates            | 41          |
|    policy_gradient_loss | 0.0204      |
|    std                  | 0.918       |
|    value_loss           | 0.864       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=24000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 24576
Recent rewards: [1.7210182746251423, 1.7210275332132976, 1.7210901578267415]
Reward changes: [9.25858816e-06 6.26246134e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 12    |
|    time_elapsed    | 13147 |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.025714524 |
|    clip_fraction        | 0.806       |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.7        |
|    explained_variance   | -0.0227     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9         |
|    n_updates            | 49          |
|    policy_gradient_loss | 0.0138      |
|    std                  | 0.902       |
|    value_loss           | 0.93        |
-----------------------------------------
Eval num_timesteps=26000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 26624
Recent rewards: [1.7210275332132976, 1.7210901578267415, 1.721030831336975]
Reward changes: [ 6.26246134e-05 -5.93264898e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 13    |
|    time_elapsed    | 14260 |
|    total_timesteps | 26624 |
------------------------------
Eval num_timesteps=27000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.024068628 |
|    clip_fraction        | 0.73        |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.1        |
|    explained_variance   | -0.0178     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.93        |
|    n_updates            | 57          |
|    policy_gradient_loss | 0.00754     |
|    std                  | 0.886       |
|    value_loss           | 1.06        |
-----------------------------------------
Eval num_timesteps=28000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 28672
Recent rewards: [1.7210901578267415, 1.721030831336975, 1.7210183143615723]
Reward changes: [-5.93264898e-05 -1.25169754e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 14    |
|    time_elapsed    | 15375 |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=29000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.026509779 |
|    clip_fraction        | 0.649       |
|    clip_range           | 0.2         |
|    entropy_loss         | 37.7        |
|    explained_variance   | -0.0217     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.87        |
|    n_updates            | 65          |
|    policy_gradient_loss | 0.00669     |
|    std                  | 0.872       |
|    value_loss           | 1.07        |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 30720
Recent rewards: [1.721030831336975, 1.7210183143615723, 1.720805565516154]
Reward changes: [-1.25169754e-05 -2.12748845e-04]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 15    |
|    time_elapsed    | 16479 |
|    total_timesteps | 30720 |
------------------------------
Eval num_timesteps=31000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.022815807 |
|    clip_fraction        | 0.551       |
|    clip_range           | 0.2         |
|    entropy_loss         | 37.9        |
|    explained_variance   | -0.0135     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.02        |
|    n_updates            | 73          |
|    policy_gradient_loss | 0.00492     |
|    std                  | 0.858       |
|    value_loss           | 1.11        |
-----------------------------------------
Eval num_timesteps=32000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 32768
Recent rewards: [1.7210183143615723, 1.720805565516154, 1.7207902669906616]
Reward changes: [-2.12748845e-04 -1.52985255e-05]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 16    |
|    time_elapsed    | 17602 |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=33000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.021459598 |
|    clip_fraction        | 0.487       |
|    clip_range           | 0.2         |
|    entropy_loss         | 34          |
|    explained_variance   | -0.0154     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.71        |
|    n_updates            | 81          |
|    policy_gradient_loss | -0.00105    |
|    std                  | 0.845       |
|    value_loss           | 0.959       |
-----------------------------------------
Eval num_timesteps=34000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 34816
Recent rewards: [1.720805565516154, 1.7207902669906616, 1.7207907438278198]
Reward changes: [-1.52985255e-05  4.76837158e-07]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 17    |
|    time_elapsed    | 18726 |
|    total_timesteps | 34816 |
------------------------------
Eval num_timesteps=35000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.021911675 |
|    clip_fraction        | 0.516       |
|    clip_range           | 0.2         |
|    entropy_loss         | 39.4        |
|    explained_variance   | -0.0134     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.95        |
|    n_updates            | 89          |
|    policy_gradient_loss | 0.00442     |
|    std                  | 0.834       |
|    value_loss           | 0.883       |
-----------------------------------------
Eval num_timesteps=36000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 36864
Recent rewards: [1.7207902669906616, 1.7207907438278198, 1.7207906246185303]
Reward changes: [ 4.76837158e-07 -1.19209290e-07]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 18    |
|    time_elapsed    | 19862 |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=37000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.023116736 |
|    clip_fraction        | 0.497       |
|    clip_range           | 0.2         |
|    entropy_loss         | 41.1        |
|    explained_variance   | -0.02       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.11        |
|    n_updates            | 97          |
|    policy_gradient_loss | 0.00245     |
|    std                  | 0.822       |
|    value_loss           | 1.05        |
-----------------------------------------
Eval num_timesteps=38000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 38912
Recent rewards: [1.7207907438278198, 1.7207906246185303, 1.7207902669906616]
Reward changes: [-1.19209290e-07 -3.57627869e-07]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 19    |
|    time_elapsed    | 20971 |
|    total_timesteps | 38912 |
------------------------------
Eval num_timesteps=39000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.018838111 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.3        |
|    explained_variance   | -0.0226     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.84        |
|    n_updates            | 105         |
|    policy_gradient_loss | -0.00318    |
|    std                  | 0.812       |
|    value_loss           | 0.82        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 40960
Recent rewards: [1.7207906246185303, 1.7207902669906616, 1.7207966248194377]
Reward changes: [-3.57627869e-07  6.35782878e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 20    |
|    time_elapsed    | 22110 |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=41000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.021152083 |
|    clip_fraction        | 0.408       |
|    clip_range           | 0.2         |
|    entropy_loss         | 40.5        |
|    explained_variance   | -0.016      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.02        |
|    n_updates            | 113         |
|    policy_gradient_loss | 0.000518    |
|    std                  | 0.802       |
|    value_loss           | 1.03        |
-----------------------------------------
Eval num_timesteps=42000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=43000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 43008
Recent rewards: [1.7207902669906616, 1.7207966248194377, 1.7207916975021362]
Reward changes: [ 6.35782878e-06 -4.92731730e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 21    |
|    time_elapsed    | 23224 |
|    total_timesteps | 43008 |
------------------------------
Eval num_timesteps=44000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 44000       |
| train/                  |             |
|    approx_kl            | 0.021057218 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.3        |
|    explained_variance   | -0.0151     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.72        |
|    n_updates            | 121         |
|    policy_gradient_loss | 0.000627    |
|    std                  | 0.792       |
|    value_loss           | 0.888       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 45056
Recent rewards: [1.7207966248194377, 1.7207916975021362, 1.720790425936381]
Reward changes: [-4.92731730e-06 -1.27156576e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 22    |
|    time_elapsed    | 24339 |
|    total_timesteps | 45056 |
------------------------------
Eval num_timesteps=46000, episode_reward=1.71 +/- 0.00
Episode length: 1.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1            |
|    mean_reward          | 1.71         |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0152055705 |
|    clip_fraction        | 0.273        |
|    clip_range           | 0.2          |
|    entropy_loss         | 38.3         |
|    explained_variance   | -0.0271      |
|    learning_rate        | 0.0001       |
|    loss                 | 1.5          |
|    n_updates            | 129          |
|    policy_gradient_loss | -0.0101      |
|    std                  | 0.783        |
|    value_loss           | 0.745        |
------------------------------------------
Eval num_timesteps=47000, episode_reward=1.71 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.71     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 47104
Recent rewards: [1.7207916975021362, 1.720790425936381, 1.7070201237996419]
Reward changes: [-1.27156576e-06 -1.37703021e-02]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 23    |
|    time_elapsed    | 25421 |
|    total_timesteps | 47104 |
------------------------------
Eval num_timesteps=48000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 48000       |
| train/                  |             |
|    approx_kl            | 0.022459634 |
|    clip_fraction        | 0.441       |
|    clip_range           | 0.2         |
|    entropy_loss         | 40.6        |
|    explained_variance   | -0.00685    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.53        |
|    n_updates            | 137         |
|    policy_gradient_loss | 0.00138     |
|    std                  | 0.775       |
|    value_loss           | 0.581       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=49000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 49152
Recent rewards: [1.720790425936381, 1.7070201237996419, 1.7211139996846516]
Reward changes: [-0.0137703   0.01409388]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 24    |
|    time_elapsed    | 26513 |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=50000, episode_reward=1.70 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.019283738 |
|    clip_fraction        | 0.307       |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.3        |
|    explained_variance   | -0.00881    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.74        |
|    n_updates            | 145         |
|    policy_gradient_loss | -0.00245    |
|    std                  | 0.766       |
|    value_loss           | 0.927       |
-----------------------------------------
Eval num_timesteps=51000, episode_reward=1.70 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 51200
Recent rewards: [1.7070201237996419, 1.7211139996846516, 1.695797085762024]
Reward changes: [ 0.01409388 -0.02531691]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 25    |
|    time_elapsed    | 27630 |
|    total_timesteps | 51200 |
------------------------------
Eval num_timesteps=52000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 52000       |
| train/                  |             |
|    approx_kl            | 0.019630358 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | 37.2        |
|    explained_variance   | -0.0124     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.94        |
|    n_updates            | 153         |
|    policy_gradient_loss | -0.000856   |
|    std                  | 0.757       |
|    value_loss           | 1.11        |
-----------------------------------------
Eval num_timesteps=53000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 53248
Recent rewards: [1.7211139996846516, 1.695797085762024, 1.7208414475123088]
Reward changes: [-0.02531691  0.02504436]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 26    |
|    time_elapsed    | 28741 |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=54000, episode_reward=1.70 +/- 0.00
Episode length: 1.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1          |
|    mean_reward          | 1.7        |
| time/                   |            |
|    total_timesteps      | 54000      |
| train/                  |            |
|    approx_kl            | 0.01997343 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | 39.9       |
|    explained_variance   | -0.0115    |
|    learning_rate        | 0.0001     |
|    loss                 | 1.93       |
|    n_updates            | 161        |
|    policy_gradient_loss | -0.00205   |
|    std                  | 0.748      |
|    value_loss           | 1.02       |
----------------------------------------
Eval num_timesteps=55000, episode_reward=1.70 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.7      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 55296
Recent rewards: [1.695797085762024, 1.7208414475123088, 1.6958283583323162]
Reward changes: [ 0.02504436 -0.02501309]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 27    |
|    time_elapsed    | 29857 |
|    total_timesteps | 55296 |
------------------------------
Eval num_timesteps=56000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 56000       |
| train/                  |             |
|    approx_kl            | 0.021549787 |
|    clip_fraction        | 0.371       |
|    clip_range           | 0.2         |
|    entropy_loss         | 37.9        |
|    explained_variance   | -0.00534    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.94        |
|    n_updates            | 169         |
|    policy_gradient_loss | -0.000567   |
|    std                  | 0.74        |
|    value_loss           | 1.01        |
-----------------------------------------
Eval num_timesteps=57000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 57344
Recent rewards: [1.7208414475123088, 1.6958283583323162, 1.72105073928833]
Reward changes: [-0.02501309  0.02522238]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 28    |
|    time_elapsed    | 30972 |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=58000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.010631015 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | 38.4        |
|    explained_variance   | -0.0224     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.84        |
|    n_updates            | 177         |
|    policy_gradient_loss | -0.0153     |
|    std                  | 0.732       |
|    value_loss           | 0.835       |
-----------------------------------------
Eval num_timesteps=59000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 59392
Recent rewards: [1.6958283583323162, 1.72105073928833, 1.720790942509969]
Reward changes: [ 0.02522238 -0.0002598 ]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 29    |
|    time_elapsed    | 32098 |
|    total_timesteps | 59392 |
------------------------------
Eval num_timesteps=60000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1           |
|    mean_reward          | 1.72        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.013356052 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | 40          |
|    explained_variance   | -0.0114     |
|    learning_rate        | 0.0001      |
|    loss                 | 1.84        |
|    n_updates            | 185         |
|    policy_gradient_loss | -0.0119     |
|    std                  | 0.724       |
|    value_loss           | 0.863       |
-----------------------------------------
Eval num_timesteps=61000, episode_reward=1.72 +/- 0.00
Episode length: 1.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1        |
|    mean_reward     | 1.72     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------

[Plateau Detected] Forcing Exploration Jump at step 61440
Recent rewards: [1.72105073928833, 1.720790942509969, 1.7207955519358318]
Reward changes: [-2.59796778e-04  4.60942586e-06]
[Exploration Jump] Env 0 triggered.
------------------------------
| time/              |       |
|    fps             | 1     |
|    iterations      | 30    |
|    time_elapsed    | 33216 |
|    total_timesteps | 61440 |
------------------------------
